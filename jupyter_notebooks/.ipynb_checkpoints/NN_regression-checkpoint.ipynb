{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "# from sentence_transformers import SentenceTransformer #https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "global surveyQuestions\n",
    "surveyQuestions = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadSurvey():\n",
    "    print(\"Load questions...\")\n",
    "    answer_map = {\n",
    "    'definitely correct': 'e',\n",
    "    'definitely incorrect': 'c',\n",
    "    'neither definitely correct nor definitely incorrect': 'n'\n",
    "    }   # use to encode responses to ANLI labels\n",
    "\n",
    "    questions = open(\"./ANLI_R3_Survey_Final.qsf\", 'r')\n",
    "    js = json.load(questions)\n",
    "\n",
    "    response_df = pd.read_csv('./ANLI_R3_Survey_Final_September 4, 2020_09.04.csv')\n",
    "    response_questions = response_df.drop(['StartDate','EndDate','Status','IPAddress','Progress','Duration (in seconds)','Finished','RecordedDate','ResponseId','RecipientLastName', \\\n",
    "    'RecipientFirstName','RecipientEmail','ExternalReference','LocationLatitude','LocationLongitude','DistributionChannel','UserLanguage'],axis=1)\n",
    "    response_questions.rename(columns={\"Random ID\": \"RandomId\"}, inplace=True)\n",
    "\n",
    "    labels = pd.read_json('./anli/r3/train.jsonl',lines=True)\n",
    "    labels.drop(['reason','uid'],axis=1,inplace=True)\n",
    "\n",
    "    for i in range(21, 319):    # the indices of the actual questions\n",
    "        qid = js['SurveyElements'][i]['PrimaryAttribute']\n",
    "        question = js['SurveyElements'][i]['Payload']['QuestionText']\n",
    "        # get rid of the template\n",
    "        question = question.replace(\"<strong>\", \"\").replace(\"</strong>\",\"\").replace(\"\"\"<span style=\"font-weight: bolder;\">Given the Premise, is the Hypothesis definitely correct, definitely incorrect, or neither? You should only choose definitely correct if the Hypothesis is clearly stated in the Premise. You should only choose definitely incorrect if the Hypothesis is clearly contradicted</span><span style=\"font-weight: bolder;\"> by the Premise.</span>\n",
    "<div><br>\"\"\", \"\").replace(\"\"\"<p><span style=\"font-weight: bolder;\"><b>\"\"\", \"\").replace(\"\"\"</b></span> <span style=\"white-space: pre-wrap;\">\"\"\",\"\").replace(\"</span></p>\",\"\").replace(\"<br><p><span><b>Hypothesis: </b></span>\",\"\").replace(\"</p>\\n</div>\",\"\").replace(\"Premise: \", \"\")\n",
    "        # separate the premise and hypothesis\n",
    "        sentences = question.split(\"\\n\")\n",
    "        sentences = [z for z in sentences if z != '']\n",
    "        premise = sentences[0]\n",
    "        hypothesis = sentences[1]\n",
    "        surveyQuestions[qid] = {'premise':premise, 'hypothesis':hypothesis, 'total_annotators':0, 'total_correct_annotators':0}\n",
    "    \n",
    "\n",
    "    # calulate how many participants got each question right\n",
    "    for id in range(len(list(response_questions.index))):\n",
    "        answers = dict(response_questions.iloc[id].dropna())\n",
    "        for k,v in answers.items():\n",
    "            try:\n",
    "                question = surveyQuestions[k]\n",
    "            except KeyError:\n",
    "                # skip attention checks\n",
    "                continue\n",
    "            try:\n",
    "                correct_label = labels[labels['hypothesis'] == question['hypothesis']]['label'].values[0]\n",
    "            except IndexError:\n",
    "                print(f\"'{question['hypothesis']}' not found\")\n",
    "                question['total_annotators'] += 1\n",
    "                question['total_correct_annotators'] += 1\n",
    "                surveyQuestions[k] = question\n",
    "                continue\n",
    "            \n",
    "            if answer_map[v] == correct_label:\n",
    "                question['total_annotators'] += 1\n",
    "                question['total_correct_annotators'] += 1\n",
    "                surveyQuestions[k] = question\n",
    "            else:\n",
    "                question['total_annotators'] += 1\n",
    "                surveyQuestions[k] = question\n",
    "        \n",
    "    print(\"here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load questions...\n",
      "'That is the kind of critic I dont like .' not found\n",
      "'That is the kind of critic I dont like .' not found\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "LoadSurvey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = surveyQuestions.keys()\n",
    "surveyQuestions_clean = {}\n",
    "for key in keys:\n",
    "    if surveyQuestions[key]['total_annotators'] != 0:\n",
    "        surveyQuestions_clean[key] = surveyQuestions[key]\n",
    "\n",
    "surveyQuestions = surveyQuestions_clean\n",
    "\n",
    "keys = surveyQuestions.keys()\n",
    "\n",
    "X = [[surveyQuestions[key]['premise'], surveyQuestions[key]['hypothesis'], float(surveyQuestions[key]['total_correct_annotators']/surveyQuestions[key]['total_annotators'])] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(X)*0.9)\n",
    "train = X[:split]\n",
    "test = X[split:]\n",
    "\n",
    "train_df = pd.DataFrame(train, columns=['text_a', 'text_b', 'labels'])\n",
    "test_df = pd.DataFrame(test, columns=['text_a', 'text_b', 'labels'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating regression models...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ANLI_roberta_vectors.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8f92fa5022d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# first create the feature set and label for each example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurveyVectorsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvecFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvecFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ANLI_roberta_vectors.jsonl'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# used to evaluate the quality of the regression\n",
    "def getSpearmanCorrelation(model):\n",
    "    # results = model.fit(X_train, y_train).predict(X_test)\n",
    "    # results = model.fit(X_train, y_train).predict(X_test)\n",
    "    y_test = [0,0,0]\n",
    "    results = [0,0,0]\n",
    "    return spearmanr(y_test, results)\n",
    "\n",
    "def getPearsonCorrelation(model):\n",
    "    results = model.fit(X_train, y_train).predict(X_test)\n",
    "    return pearsonr(y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
